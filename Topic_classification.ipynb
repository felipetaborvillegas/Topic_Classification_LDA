{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e77a41-b8bd-4a78-b2c8-d970e3b7dabd",
   "metadata": {},
   "source": [
    "# \"Text Classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd17d86-af54-4537-b880-17091ea84a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages \n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce8e05b-3ae1-4a69-ae2a-d9ea4208524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/felipe/.local/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: jinja2 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/felipe/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/felipe/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /home/felipe/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/felipe/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felipe/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/felipe/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/felipe/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/felipe/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/felipe/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/felipe/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/felipe/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/felipe/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/felipe/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.7.0\n",
      "    Uninstalling en-core-web-sm-3.7.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.7.0\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Extra installations\n",
    "\n",
    "!python3.10 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc15b53-0644-49e3-8b4b-38fa08748b6b",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee09b19-cea3-4fe8-a69e-9baa2ab90717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing your text files\n",
    "directory_path = './data/'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data_list = []\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Read the text file and append its content to the list\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            data_list.append({'Filename': filename, 'Content': content})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de108f6c-7422-416d-b3f5-062b0538a944",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8baed62-86c6-41e5-b9ad-9b0c579c5121",
   "metadata": {},
   "source": [
    "We will use **Latent Dirichlet Allocation (LDA)** is an unsupervised technique for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7ccbbc-5536-4ce6-9fa5-e48e60912802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hyperparams\n",
    "\n",
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "NUM_TOPICS = 10 # This number is a personal assumption may variate according to the results\n",
    "sp = spacy.load(\"en_core_web_sm\") # tokenizer\n",
    "\n",
    "# reproducibility\n",
    "\n",
    "np.random.seed(12)\n",
    "random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6330b2f-5feb-40ff-b767-929be783eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(data, max_docs=10000):\n",
    "    \"\"\"This function allows us to know the frequency of a word in a text\"\"\"\n",
    "    freqs = Counter()\n",
    "    all_stopwords = sp.Defaults.stop_words # Words that don't add any special value to the classification process\n",
    "    # all_stopwords.add() --> to add words that we consider do not add value\n",
    "    nr_tokens = 0\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "        for token in tokens:\n",
    "            token_text = token.text.lower()\n",
    "            if token_text not in all_stopwords and token.is_alpha:\n",
    "                nr_tokens += 1\n",
    "                freqs[token_text] += 1\n",
    "                \n",
    "    return freqs\n",
    "    \n",
    "def get_vocab(freqs, freq_threshold=3):\n",
    "    \"\"\"\n",
    "    Select the words with more than 3 appearances on a text and \n",
    "    add the to the vocabulary\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    vocab_idx_str = {}\n",
    "    vocab_idx = 0\n",
    "\n",
    "    for word in freqs:\n",
    "        if freqs[word] >= freq_threshold:\n",
    "            vocab[word] = vocab_idx\n",
    "            vocab_idx_str[vocab_idx] = word\n",
    "            vocab_idx += 1\n",
    "\n",
    "    return vocab, vocab_idx_str\n",
    "\n",
    "def tokenize_dataset(data, vocab, max_docs=10000):\n",
    "    nr_tokens = 0\n",
    "    nr_docs = 0\n",
    "    docs = []\n",
    "\n",
    "    for doc in data[:max_docs]:\n",
    "        tokens = sp.tokenizer(doc)\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            doc = []\n",
    "            for token in tokens:\n",
    "                token_text = token.text.lower()\n",
    "                if token_text in vocab:\n",
    "                    doc.append(token_text)\n",
    "                    nr_tokens += 1\n",
    "            nr_docs += 1\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"Number of text: {nr_docs}\")\n",
    "    print(f\"Number of tokens: {nr_tokens}\")\n",
    "\n",
    "    # Numericalize\n",
    "    corpus = []\n",
    "    for doc in docs:\n",
    "        corpus_d = []\n",
    "\n",
    "        for token in doc:\n",
    "            corpus_d.append(vocab[token])\n",
    "\n",
    "        corpus.append(np.asarray(corpus_d))\n",
    "\n",
    "    return docs, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c538a273-2f49-4819-8c20-1afc2c6905b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text: 3990\n",
      "Number of tokens: 1827365\n",
      "Vocab size: 22282\n"
     ]
    }
   ],
   "source": [
    "data = df[\"Content\"].sample(frac=0.3, random_state=12).values\n",
    "freqs = generate_frequencies(data)\n",
    "vocab, vocab_idx_str = get_vocab(freqs)\n",
    "docs, corpus = tokenize_dataset(data, vocab)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd373e8b-5943-4779-aab2-7fd5d0014f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [53:56<00:00, 16.18s/it]\n"
     ]
    }
   ],
   "source": [
    "def LDA_Collapsed_Gibbs(corpus, num_iter=200):\n",
    "    # Initialize counts and Z\n",
    "    Z = []\n",
    "    num_docs = len(corpus)\n",
    "    for _, doc in enumerate(corpus):\n",
    "        Zd = np.random.randint(low=0, high=NUM_TOPICS, size=(len(doc)))\n",
    "        Z.append(Zd)\n",
    "\n",
    "    ndk = np.zeros((num_docs, NUM_TOPICS))\n",
    "    for d in range(num_docs):\n",
    "        for k in range(NUM_TOPICS):\n",
    "            ndk[d, k] = np.sum(Z[d] == k)\n",
    "\n",
    "    nkw = np.zeros((NUM_TOPICS, vocab_size))\n",
    "    for doc_idx, doc in enumerate(corpus):\n",
    "        for i, word in enumerate(doc):\n",
    "            topic = Z[doc_idx][i]\n",
    "            nkw[topic, word] += 1        \n",
    "\n",
    "    nk = np.sum(nkw, axis=1)\n",
    "    topic_list = [i for i in range(NUM_TOPICS)]\n",
    "    \n",
    "    # Loop\n",
    "    for _ in tqdm(range(num_iter)):\n",
    "        for doc_idx, doc in enumerate(corpus):\n",
    "            for i in range(len(doc)):\n",
    "                word = doc[i]\n",
    "                topic = Z[doc_idx][i]\n",
    "\n",
    "                # remove z_i because conditioned on z_(-i)\n",
    "                ndk[doc_idx, topic] -= 1\n",
    "                nkw[topic, word] -= 1\n",
    "                nk[topic] -= 1\n",
    "\n",
    "                p_z = (ndk[doc_idx, :] + ALPHA) * (nkw[:, word] + BETA) / (nk[:] + BETA*vocab_size)\n",
    "                topic = random.choices(topic_list, weights=p_z, k=1)[0]\n",
    "\n",
    "                # update n parametersdoc\n",
    "                Z[doc_idx][i] = topic\n",
    "                ndk[doc_idx, topic] += 1\n",
    "                nkw[topic, word] += 1\n",
    "                nk[topic] += 1\n",
    "\n",
    "    return Z, ndk, nkw, nk\n",
    "\n",
    "Z, ndk, nkw, nk = LDA_Collapsed_Gibbs(corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87566403-381d-4256-9a2c-e8640ac02702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 most common words: \n",
      "research\n",
      "materials\n",
      "text\n",
      "energy\n",
      "project\n",
      "programreference\n",
      "high\n",
      "related\n",
      "amp\n",
      "new\n",
      "\n",
      "\n",
      "Topic 1 most common words: \n",
      "research\n",
      "related\n",
      "amp\n",
      "award\n",
      "quantum\n",
      "text\n",
      "mathematical\n",
      "project\n",
      "division\n",
      "longname\n",
      "\n",
      "\n",
      "Topic 2 most common words: \n",
      "data\n",
      "research\n",
      "project\n",
      "text\n",
      "amp\n",
      "learning\n",
      "related\n",
      "programreference\n",
      "systems\n",
      "nsf\n",
      "\n",
      "\n",
      "Topic 3 most common words: \n",
      "research\n",
      "ocean\n",
      "project\n",
      "related\n",
      "text\n",
      "nsf\n",
      "earth\n",
      "united\n",
      "directorate\n",
      "award\n",
      "\n",
      "\n",
      "Topic 4 most common words: \n",
      "research\n",
      "water\n",
      "project\n",
      "text\n",
      "environmental\n",
      "related\n",
      "climate\n",
      "programreference\n",
      "united\n",
      "change\n",
      "\n",
      "\n",
      "Topic 5 most common words: \n",
      "project\n",
      "sars\n",
      "phase\n",
      "virus\n",
      "div\n",
      "p\n",
      "technology\n",
      "modified\n",
      "system\n",
      "health\n",
      "\n",
      "\n",
      "Topic 6 most common words: \n",
      "div\n",
      "p\n",
      "images\n",
      "research\n",
      "reports\n",
      "modified\n",
      "conference\n",
      "students\n",
      "workshop\n",
      "li\n",
      "\n",
      "\n",
      "Topic 7 most common words: \n",
      "research\n",
      "text\n",
      "biological\n",
      "project\n",
      "related\n",
      "programreference\n",
      "species\n",
      "award\n",
      "font\n",
      "cell\n",
      "\n",
      "\n",
      "Topic 8 most common words: \n",
      "students\n",
      "research\n",
      "stem\n",
      "project\n",
      "education\n",
      "science\n",
      "text\n",
      "learning\n",
      "investigator\n",
      "lastname\n",
      "\n",
      "\n",
      "Topic 9 most common words: \n",
      "social\n",
      "research\n",
      "project\n",
      "data\n",
      "pandemic\n",
      "health\n",
      "public\n",
      "economic\n",
      "study\n",
      "related\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phi = nkw / nk.reshape(NUM_TOPICS,1) # To get the probability distribution\n",
    "\n",
    "num_words = 10\n",
    "for k in range(NUM_TOPICS):\n",
    "    most_common_words = np.argsort(phi[k])[::-1][:num_words]\n",
    "    print(f\"Topic {k} most common words: \")\n",
    "\n",
    "    for word in most_common_words:\n",
    "        print(vocab_idx_str[word])\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a67e67-6368-41b2-96dc-28989ded1b31",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9014c176-eaa4-47dc-a71d-4e5fa76e2c2d",
   "metadata": {},
   "source": [
    "Apriori to the execution of the model we supposed that the articles could be classified in \n",
    "10 groups. This is a relevant parameter which it's left to the decision of the analyst.\n",
    "\n",
    "Output explanation: The output consists of 10 topics and for each of them the ten most common words are displayed.\n",
    "\n",
    "Now, it's up to our criteria to determine what are the topics, based on the words related to each of them.\n",
    "\n",
    "The proposed classification is:\n",
    "\n",
    "Topic 0 --> Engineering\n",
    "Topic 1 --> Mathematics / Physics\n",
    "Topic 2 --> NOT CLEAR\n",
    "Topic 3 --> NOT CLEAR\n",
    "Topic 4 --> Environment / Ecology\n",
    "Topic 5 --> Medicine\n",
    "Topic 6 --> NOT CLEAR\n",
    "Topic 7 --> Biology\n",
    "Topic 8 --> Education\n",
    "Topic 9 --> Economics\n",
    "\n",
    "To improve this classification we have multiple options:\n",
    "\n",
    "* Use more texts to train the model but this will be time-consuming or more computational resources would be needed.\n",
    "* Modified the number of categories at the beginning.\n",
    "* Include words such as \"research\", \"project\" and \"awards\" in the \"all_stopwords\" list because they are repeated in all\n",
    "the topics but do not add any value to the classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDATA",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
